{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Multiclass Image Classification Example\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. Data Preperation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our end-to-end example of distributed image classification algorithm. In this demo, we will use the Amazon sagemaker image classification algorithm to train on the [caltech-256 dataset](http://www.vision.caltech.edu/Image_Datasets/Caltech256/). \n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. First we get the preconfigured training images. Training images are basically docker containers with necessary ML library and Pythin installations ready to use/reuse for your own training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'caltech-256-data'\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Download the data and transfer to S3 for use in training. We will use apache's rec format for speeding up the training. It is already created in the Caltech site so we will transfer it to S3 bucket for further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caltech-256\n",
    "!wget -N http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec\n",
    "train_file = 'caltech-256-60-train.rec'\n",
    "\n",
    "!wget -N http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec\n",
    "validation_file = 'caltech-256-60-val.rec'"
   ]
  },
  {
   "source": [
    "Upload data to S3 for further analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "validation_data_s3_path = session.upload_data(path=validation_file, key_prefix=prefix + \"/validation\")\n",
    "print('Please see: {} for your newly updated training and {} for your validation data'.format(bucket+'/'+prefix+'/train',bucket+'/'+prefix+'/validation'))"
   ]
  },
  {
   "source": [
    "you have completed this section. Please follow the next step in the README.md file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}